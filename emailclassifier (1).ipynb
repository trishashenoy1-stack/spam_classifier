{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5tMMa9_xljl"
      },
      "outputs": [],
      "source": [
        "!pip install gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import Word2Vec\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from sklearn.utils import resample\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "!pip install pytorch-lightning\n",
        "import pytorch_lightning as L\n",
        "from torch.optim import Adam\n",
        "import torchmetrics\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "import torchmetrics\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MKVCMXGqfMH"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('spam_ham_dataset.csv')\n",
        "df.head()\n",
        "df['cap']=df['text'].str.count(r'[A-Z]')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words=set(stopwords.words('english'))\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "G9_wnhOqdZfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop(text):\n",
        "  tokens=word_tokenize(text)\n",
        "  filtered_tokens=[word for word in tokens if word.lower() not in stop_words]\n",
        "  filtered_text=' '.join(filtered_tokens)\n",
        "  return filtered_text"
      ],
      "metadata": {
        "id": "I0BYQKJRf4tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def lem(text):\n",
        "  tokens=word_tokenize(text)\n",
        "  lemmatized_tokens=[lemmatizer.lemmatize(word) for word in tokens]\n",
        "  lemmatized_text=' '.join(lemmatized_tokens)\n",
        "  return lemmatized_text\n"
      ],
      "metadata": {
        "id": "tdYp0gapgPth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrfanFm9rHhh"
      },
      "outputs": [],
      "source": [
        "splitdata=df['text'].str.split('\\n',expand=True)\n",
        "df['subject']=splitdata[0]\n",
        "df['body']=splitdata[1]\n",
        "df['subject']=df['subject'].str.lower()\n",
        "df['body']=df['body'].str.lower()\n",
        "df['!']=df['text'].str.count('!')\n",
        "df['subject']=df['subject'].str.replace(\"subject:\", \"\", regex=False).str.strip()\n",
        "df['url']=df['text'].str.count(r'http|www|\\.com')\n",
        "df['digit'] = df['text'].str.count(r'\\d')\n",
        "def vocabratio(text):\n",
        "    words = text.lower().split()\n",
        "    if len(words) == 0:\n",
        "        return 0\n",
        "    return len(set(words))/len(words)\n",
        "df['vocabratio'] = df['text'].apply(vocabratio) #helps check the repetition of words\n",
        "df['wordcount'] = df['text'].apply(lambda x: len(str(x).split()))\n",
        "df['text'] = df['text'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
        "df['subject'] = df['text'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0A97T9IkgiEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7lImn_64B5U"
      },
      "outputs": [],
      "source": [
        "features = [ '!','url','digit','vocabratio']\n",
        "\n",
        "fig,axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(features):\n",
        "    if col in df.columns:\n",
        "        sns.boxplot(x='label', y=col, data=df, ax=axes[i], palette=\"Set2\", showfliers=False)\n",
        "\n",
        "        axes[i].set_title(f'Feature: {col}', fontsize=12, fontweight='bold')\n",
        "        axes[i].grid(axis='y', linestyle='--', alpha=0.3)\n",
        "    else:\n",
        "        axes[i].set_visible(False)\n",
        "\n",
        "plt.suptitle('Feature Comparison', fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WVA4zxY6sb-"
      },
      "outputs": [],
      "source": [
        "vectorizer=CountVectorizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfgeYwrZ-jxw"
      },
      "outputs": [],
      "source": [
        "df_train,df_test=train_test_split(df,test_size=0.2,random_state=42)\n",
        "\n",
        "\n",
        "X_train_text=df_train['text']\n",
        "X_test_text=df_test['text']\n",
        "\n",
        "\n",
        "\n",
        "feature_cols=['cap','!','url','digit','vocabratio']\n",
        "\n",
        "X_train_features=df_train[feature_cols]\n",
        "X_test_features=df_test[feature_cols]\n",
        "\n",
        "X_train_count=vectorizer.fit_transform(X_train_text)\n",
        "X_test_count=vectorizer.transform(X_test_text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "y_train=df_train['label_num']\n",
        "y_test=df_test['label_num']\n",
        "nb=MultinomialNB()\n",
        "nb.fit(X_train_count,y_train)\n",
        "\n",
        "train_bayes=nb.predict(X_train_count)\n",
        "test_bayes=nb.predict(X_test_count)\n",
        "\n",
        "X_train_features = X_train_features.copy()\n",
        "X_train_features.loc[:,'bayes']=train_bayes\n",
        "\n",
        "X_test_features=X_test_features.copy()\n",
        "X_test_features.loc[:,'bayes']=test_bayes\n",
        "print(f\"Train Text: {len(X_train_text)}\")\n",
        "print(f\"Train Feats: {len(X_train_features)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5Ak1z2b69cQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7zg3ApnohLZ"
      },
      "outputs": [],
      "source": [
        "rus = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
        "#downsampling the data\n",
        "X_train_text_df = X_train_text.to_frame(name='text_content')\n",
        "\n",
        "combined_X_train=pd.concat([X_train_features,X_train_text_df], axis=1)\n",
        "resampled_combined_X_train,resampled_y_train=rus.fit_resample(combined_X_train, y_train)\n",
        "X_train_features=resampled_combined_X_train[X_train_features.columns]\n",
        "X_train_text=resampled_combined_X_train['text_content']\n",
        "y_train=resampled_y_train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y89lGW9vdsLN"
      },
      "outputs": [],
      "source": [
        "train_sentences = [text.split() for text in X_train_text]\n",
        "\n",
        "w2v_model = Word2Vec(sentences=train_sentences, vector_size=300,  window=5, min_count=1, workers=4)\n",
        "#each vector(word) has 300 values, window means how many words left and right the word looks to understand,\n",
        "\n",
        "#w2v uses a shallow nn to train\n",
        "#similar numbers have small euclidean distance\n",
        "MAX_LEN=100 #100 words per email\n",
        "EMBED_DIM=300 #300 numbers per word\n",
        "#building the 3d tensor\n",
        "\n",
        "def text_to_vectors(text_series, model, max_len, embed_dim):\n",
        "    num_samples=len(text_series)#number of emails\n",
        "    matrix=np.zeros((num_samples, max_len, embed_dim),dtype=np.float32)#len=num_samples(number of emails), width=max_len(number of words per mail)\n",
        "    #iterating through the words\n",
        "    for i, text in enumerate(text_series):\n",
        "        words=text.split()#tokenizes the sentence\n",
        "        for t, word in enumerate(words): #t is time step tacks position of word\n",
        "            if t>=max_len:#if it crosses 100 words\n",
        "                break\n",
        "            #basically making sure the model skips over unknown words\n",
        "            if word in model.wv:\n",
        "                matrix[i, t, :] = model.wv[word] #email i, word t, paste the 300 word vector into that slot\n",
        "\n",
        "    return matrix\n",
        "\n",
        "X_train_vectors=text_to_vectors(X_train_text, w2v_model, MAX_LEN, EMBED_DIM)\n",
        "X_test_vectors=text_to_vectors(X_test_text, w2v_model, MAX_LEN, EMBED_DIM)\n",
        "print(f\"Final Train Shape: {X_train_vectors.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npGBgJQn7bRE"
      },
      "outputs": [],
      "source": [
        "class SpamDataset(Dataset):\n",
        "  #class acts as a container , bundles the data together and adds functions\n",
        "    def __init__(self, text_tensor, feature_tensor, label_tensor):\n",
        "\n",
        "      #expects 3 features- 3d text, 2d feautes, 1d classification label\n",
        "        self.text = torch.tensor(text_tensor)\n",
        "        self.features = torch.tensor(feature_tensor, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(label_tensor, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self): #how many emils, to define when an epoch os finished\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx): #grabs that id number and puts the text features and label into a tuple\n",
        "\n",
        "        return self.text[idx], self.features[idx], self.labels[idx]\n",
        "\n",
        "train_dataset=SpamDataset(X_train_vectors, X_train_features.values, y_train.values)\n",
        "test_dataset=SpamDataset(X_test_vectors, X_test_features.values, y_test.values)\n",
        "#x train vectors is 4000*100*300\n",
        "train_loader=DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader=DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGExzwaLYNln"
      },
      "outputs": [],
      "source": [
        "class lightninglstm(L.LightningModule):\n",
        "  def __init__(self, embed_dim, num_features, lstm_hidden_size=128, lstm_num_layers=2,bidirectional=True):\n",
        "    #embed dim=size of word vec= 300num features= engineered columns and added features, hidden size=no. of numbers lstm can hold\n",
        "    super().__init__()\n",
        "    self.lstm=nn.LSTM(input_size=embed_dim,hidden_size=lstm_hidden_size,num_layers=lstm_num_layers,batch_first=True,bidirectional=True)\n",
        "    #making it biirectional reads l to r and r to l\n",
        "    #basically 2 hidden layers works like 2 separate lstms\n",
        "\n",
        "\n",
        "    #telling lstm to expect 300 words as the input, btacfirst= true says that the input is [batch,sequence,features] where batch is basically features,+classification\n",
        "\n",
        "    self.fc=nn.Linear(lstm_hidden_size*2+num_features,1)#fully connected layer,merges my features with lstm memory,\n",
        "    self.sigmoid=nn.Sigmoid() #ensures final score is between 0 and 1 d=for sigmoid\n",
        "\n",
        "    self.criterion=nn.BCELoss() # Binary Cross-Entropy Loss for sigmoid output\n",
        "    self.accuracy=torchmetrics.Accuracy(task='binary') #calculates by correct/total\n",
        "\n",
        "  def forward(self, text_input, feature_input):#accepts text vectors and features separately\n",
        "    lstm_out, (hidden, cell) = self.lstm(text_input)#lstm_out basically has what the model thought at each word,\n",
        "    diff_hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1) #use the hidden state from the last layer\n",
        "\n",
        "    combined_features=torch.cat((diff_hidden, feature_input), dim=1)#combines its final hidden state(whatever it thinks) with my features\n",
        "    output=self.fc(combined_features) #outputs a single rawscore(logit)\n",
        "    prediction=self.sigmoid(output) #converts that to a probability\n",
        "    return prediction.squeeze(1)#converts col vectir to a flat list\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = Adam(self.parameters(), lr=0.001)\n",
        "    return optimizer\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    batch_text,batch_features,batch_y=batch #unpacks the text, features and final class\n",
        "    batch_y=batch_y.float()\n",
        "    output=self.forward(batch_text, batch_features)\n",
        "    loss = self.criterion(output, batch_y)\n",
        "    self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True) #records the data and outputs etc\n",
        "    return loss\n",
        "\n",
        "  def validation_step(self, batch, batch_idx): #checks whether the model is actually learning or not - recieves a batch of data it has not trained before\n",
        "    batch_text,batch_features, batch_y=batch\n",
        "    batch_y=batch_y.float()\n",
        "    output=self.forward(batch_text, batch_features)#pass both text and features\n",
        "    loss=self.criterion(output, batch_y)#calculates bce\n",
        "    preds=(output > 0.8).float() #converts into true/false\n",
        "    acc=self.accuracy(preds, batch_y)\n",
        "    self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "    self.log(\"val_acc\", acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "    return loss\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "    batch_text, batch_features, batch_y = batch\n",
        "    batch_y = batch_y.float()\n",
        "    output = self.forward(batch_text, batch_features)\n",
        "    loss = self.criterion(output, batch_y)\n",
        "    preds = (output > 0.5).float()\n",
        "    acc = self.accuracy(preds, batch_y)\n",
        "    self.log(\"test_loss\", loss, on_step=False, on_epoch=True, logger=True)\n",
        "    self.log(\"test_acc\", acc, on_step=False, on_epoch=True, logger=True)\n",
        "    return {'test_preds': preds, 'test_labels': batch_y} #returns prediction vs label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljDRNNrziqcx"
      },
      "outputs": [],
      "source": [
        "num_features_additional=X_train_features.shape[1] #gets the numbers of features\n",
        "\n",
        "model=lightninglstm(embed_dim=EMBED_DIM, num_features=num_features_additional)\n",
        "\n",
        "\n",
        "early_stop_callback = EarlyStopping(monitor='val_loss',min_delta=0.00,patience=3,verbose=False,mode='min')\n",
        "#use valloss to see o=hoew model performs on  unseen data, patience= how many bad epochs the model can tolerate, mode=min mmeans reduce error\n",
        "\n",
        "\n",
        "trainer = L.Trainer(max_epochs=50, callbacks=[early_stop_callback]) #trainer has epochs, optimizer etc already\n",
        "trainer.fit(model, train_loader, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpCSPWNn7F5Z"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_text,batch_features, batch_y in test_loader:\n",
        "        batch_text=batch_text.to(model.device)\n",
        "        batch_features=batch_features.to(model.device)\n",
        "\n",
        "        logits=model(batch_text,batch_features) #calculated probabilities\n",
        "\n",
        "        preds=(logits > 0.8).float() #setting the threshold\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(batch_y.cpu().numpy())\n",
        "\n",
        "\n",
        "print(classification_report(all_labels, all_preds, target_names=['Ham', 'Spam']))\n",
        "\n",
        "#here precision- when the model says spam is it actually spam?, recall is how many of the spams did it catch\n",
        "#precision more imp because what is imp mail is falsely classified as spam\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def vocabratio(text):\n",
        "    words=text.lower().split()\n",
        "    if len(words)==0:\n",
        "        return 0\n",
        "    return len(set(words))/len(words)\n",
        "\n",
        "def text_to_vectors_single(text_content,w2v_model,max_len,embed_dim):\n",
        "    matrix=np.zeros((1,max_len,embed_dim),dtype=np.float32)\n",
        "    words=text_content.split()\n",
        "    for t,word in enumerate(words):\n",
        "        if t>=max_len:\n",
        "            break\n",
        "        if word in w2v_model.wv:\n",
        "            matrix[0,t,:]=w2v_model.wv[word]\n",
        "    return matrix\n",
        "\n",
        "def predict_spam(model,raw_text,w2v_model,count_vectorizer,naive_bayes_model,max_len=100,embed_dim=300,threshold=0.8):\n",
        "\n",
        "    lower_text=raw_text.lower()\n",
        "\n",
        "    cap_feature=len(re.findall(r'[A-Z]', raw_text))\n",
        "    exclamation_feature=raw_text.count('!')\n",
        "    url_feature=len(re.findall(r'http|www|\\.com', lower_text))\n",
        "    digit_feature=len(re.findall(r'\\d', raw_text))\n",
        "    vocab_ratio_feature=vocabratio(raw_text)\n",
        "    text_for_bayes=re.sub(r'[^\\w\\s]', ' ', lower_text)\n",
        "\n",
        "    count_vector_single=count_vectorizer.transform([text_for_bayes])\n",
        "    bayes_prediction=naive_bayes_model.predict(count_vector_single)[0]\n",
        "\n",
        "\n",
        "    features_list=[cap_feature, exclamation_feature, url_feature, digit_feature, vocab_ratio_feature, bayes_prediction]\n",
        "    feature_tensor=torch.tensor([features_list], dtype=torch.float32)\n",
        "\n",
        "    text_for_w2v=re.sub(r'[^\\w\\s]', ' ', lower_text)\n",
        "    text_tensor=torch.tensor(text_to_vectors_single(text_for_w2v, w2v_model, max_len, embed_dim))\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    text_tensor=text_tensor.to(model.device)\n",
        "    feature_tensor=feature_tensor.to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        probability=model(text_tensor, feature_tensor)\n",
        "\n",
        "    score=probability.item()\n",
        "    label=\"SPAM\" if score > threshold else \"HAM\"\n",
        "\n",
        "    return label, score\n",
        "\n",
        "new_email=\"\"\"\n",
        "\n",
        "Subject: MESS & ANC FOR THE MONTH OF JANUARY 2026\n",
        "\n",
        "\n",
        "Dear All,\n",
        "\n",
        "Dear Students,\n",
        "The Mess and ANC bills for the month of January 2026  have been uploaded on the SWD portal.\n",
        "\n",
        "You are requested to kindly review your mess & Anc  bills at the earliest. If you notice any discrepancies or have any concerns, please contact me before 3.00 PM on 06.02.2026..\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prediction, confidence = predict_spam(model, new_email, w2v_model, vectorizer, nb, threshold=0.8)\n",
        "\n",
        "print(f\"Email: {new_email}\")\n",
        "print(f\"Prediction: {prediction}\")\n",
        "print(f\"Confidence Score: {confidence:.4f}\")"
      ],
      "metadata": {
        "id": "_ewCb8btpDrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Go-kxT0soDO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}